{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eaac7d50-54af-47cb-bd6b-08a910f5a468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset loaded successfully.\n",
      "Dataset Shape: (91713, 186)\n",
      "\n",
      "Data Preview:\n",
      "   encounter_id  patient_id  hospital_id  hospital_death   age    bmi  \\\n",
      "0         66154       25312          118               0  68.0  22.73   \n",
      "1        114252       59342           81               0  77.0  27.42   \n",
      "2        119783       50777          118               0  25.0  31.95   \n",
      "3         79267       46918          118               0  81.0  22.64   \n",
      "4         92056       34377           33               0  19.0    NaN   \n",
      "\n",
      "   elective_surgery  ethnicity gender  height  ... aids cirrhosis  \\\n",
      "0                 0  Caucasian      M   180.3  ...  0.0       0.0   \n",
      "1                 0  Caucasian      F   160.0  ...  0.0       0.0   \n",
      "2                 0  Caucasian      F   172.7  ...  0.0       0.0   \n",
      "3                 1  Caucasian      F   165.1  ...  0.0       0.0   \n",
      "4                 0  Caucasian      M   188.0  ...  0.0       0.0   \n",
      "\n",
      "   diabetes_mellitus hepatic_failure immunosuppression  leukemia  lymphoma  \\\n",
      "0                1.0             0.0               0.0       0.0       0.0   \n",
      "1                1.0             0.0               0.0       0.0       0.0   \n",
      "2                0.0             0.0               0.0       0.0       0.0   \n",
      "3                0.0             0.0               0.0       0.0       0.0   \n",
      "4                0.0             0.0               0.0       0.0       0.0   \n",
      "\n",
      "   solid_tumor_with_metastasis  apache_3j_bodysystem  apache_2_bodysystem  \n",
      "0                          0.0                Sepsis       Cardiovascular  \n",
      "1                          0.0           Respiratory          Respiratory  \n",
      "2                          0.0             Metabolic            Metabolic  \n",
      "3                          0.0        Cardiovascular       Cardiovascular  \n",
      "4                          0.0                Trauma               Trauma  \n",
      "\n",
      "[5 rows x 186 columns]\n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 91713 entries, 0 to 91712\n",
      "Columns: 186 entries, encounter_id to apache_2_bodysystem\n",
      "dtypes: float64(170), int64(8), object(8)\n",
      "memory usage: 130.1+ MB\n",
      "None\n",
      "Cleaned dataset loaded successfully.\n",
      "Dictionary Shape: (188, 6)\n",
      "\n",
      "Data Preview:\n",
      "      Category   Variable Name Unit of Measure Data Type  \\\n",
      "0   identifier    encounter_id             NaN   integer   \n",
      "1   identifier     hospital_id             NaN   integer   \n",
      "2   identifier      patient_id             NaN   integer   \n",
      "3  demographic  hospital_death             NaN    binary   \n",
      "4  demographic             age           Years   numeric   \n",
      "\n",
      "                                         Description Example  \n",
      "0  Unique identifier associated with a patient un...     NaN  \n",
      "1       Unique identifier associated with a hospital     NaN  \n",
      "2        Unique identifier associated with a patient     NaN  \n",
      "3  Whether the patient died during this hospitali...       0  \n",
      "4           The age of the patient on unit admission     NaN  \n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188 entries, 0 to 187\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Category         188 non-null    object\n",
      " 1   Variable Name    188 non-null    object\n",
      " 2   Unit of Measure  142 non-null    object\n",
      " 3   Data Type        188 non-null    object\n",
      " 4   Description      187 non-null    object\n",
      " 5   Example          178 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 8.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "data_path = \"D:/Datathon 3/Datathon3_GOSSIS_MIT.csv\"\n",
    "dictionary_path = \"D:/Datathon 3/Datathon3_GOSSIS_MIT_Data Dictionary.csv\"\n",
    "# Load the dataset\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# load the data dictionary \n",
    "dictionary_df = pd.read_csv(dictionary_path)\n",
    "\n",
    "\n",
    "# Confirm successful load\n",
    "print(\"Cleaned dataset loaded successfully.\")\n",
    "print(f\"Dataset Shape: {df.shape}\")  # Check rows & columns\n",
    "print(\"\\nData Preview:\")\n",
    "print(df.head())  # Display first 5 rows\n",
    "\n",
    "# Check data types and missing values\n",
    "print(\"\\nData Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Confirm successful load\n",
    "print(\"Cleaned dataset loaded successfully.\")\n",
    "print(f\"Dictionary Shape: {dictionary_df.shape}\")  # Check rows & columns\n",
    "print(\"\\nData Preview:\")\n",
    "print(dictionary_df.head())  # Display first 5 rows\n",
    "\n",
    "# Check data types and missing values\n",
    "print(\"\\nData Info:\")\n",
    "print(dictionary_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76330158-b37f-46ce-b83a-3bf032c8dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['identifier' 'demographic' 'APACHE covariate' 'vitals' 'labs'\n",
      " 'labs blood gas' 'APACHE prediction' 'APACHE comorbidity'\n",
      " 'APACHE grouping' 'GOSSIS example prediction']\n"
     ]
    }
   ],
   "source": [
    "unique_vals = dictionary_df['Category'].unique()\n",
    "print(unique_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e8f51ad-7422-412a-8f1b-845df2b325dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape after keep: (91713, 146)\n"
     ]
    }
   ],
   "source": [
    "# Keep all categories of coulmns except APACHE 'APACHE prediction' 'APACHE comorbidity'\n",
    "# 'APACHE grouping' 'GOSSIS example prediction'\n",
    "\n",
    "categories_to_keep = [\"identifier\",\"demographic\", \"vitals\",\"labs\",\"labs blood gas\"]\n",
    "\n",
    "# Get all columns that have categories not in categories_to_keep\n",
    "cols_to_drop = dictionary_df.loc[\n",
    "    ~dictionary_df[\"Category\"].isin(categories_to_keep),  # \"~\" means \"NOT in\"\n",
    "    \"Variable Name\"\n",
    "].tolist()\n",
    "\n",
    "df = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "print(\"df shape after keep:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a6a3a3df-b430-4eef-9f15-7e40dd5240e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Preview:\n",
      "   encounter_id  patient_id  hospital_id  hospital_death   age    bmi  \\\n",
      "0         66154       25312          118               0  68.0  22.73   \n",
      "1        114252       59342           81               0  77.0  27.42   \n",
      "2        119783       50777          118               0  25.0  31.95   \n",
      "3         79267       46918          118               0  81.0  22.64   \n",
      "4         92056       34377           33               0  19.0    NaN   \n",
      "\n",
      "   elective_surgery  ethnicity gender  height  ... d1_pao2fio2ratio_max  \\\n",
      "0                 0  Caucasian      M   180.3  ...                  NaN   \n",
      "1                 0  Caucasian      F   160.0  ...                 54.8   \n",
      "2                 0  Caucasian      F   172.7  ...                  NaN   \n",
      "3                 1  Caucasian      F   165.1  ...                342.5   \n",
      "4                 0  Caucasian      M   188.0  ...                  NaN   \n",
      "\n",
      "  d1_pao2fio2ratio_min  h1_arterial_pco2_max h1_arterial_pco2_min  \\\n",
      "0                  NaN                   NaN                  NaN   \n",
      "1            51.000000                  37.0                 37.0   \n",
      "2                  NaN                   NaN                  NaN   \n",
      "3           236.666667                  36.0                 33.0   \n",
      "4                  NaN                   NaN                  NaN   \n",
      "\n",
      "  h1_arterial_ph_max  h1_arterial_ph_min  h1_arterial_po2_max  \\\n",
      "0                NaN                 NaN                  NaN   \n",
      "1               7.45                7.45                 51.0   \n",
      "2                NaN                 NaN                  NaN   \n",
      "3               7.37                7.34                337.0   \n",
      "4                NaN                 NaN                  NaN   \n",
      "\n",
      "   h1_arterial_po2_min  h1_pao2fio2ratio_max  h1_pao2fio2ratio_min  \n",
      "0                  NaN                   NaN                   NaN  \n",
      "1                 51.0                  51.0                  51.0  \n",
      "2                  NaN                   NaN                   NaN  \n",
      "3                265.0                 337.0                 337.0  \n",
      "4                  NaN                   NaN                   NaN  \n",
      "\n",
      "[5 rows x 146 columns]\n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 91713 entries, 0 to 91712\n",
      "Columns: 146 entries, encounter_id to h1_pao2fio2ratio_min\n",
      "dtypes: float64(133), int64(7), object(6)\n",
      "memory usage: 102.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    " \n",
    "print(\"\\nData Preview:\")\n",
    "print(df.head())  # Display first 5 rows\n",
    "\n",
    "# Check data types and missing values\n",
    "print(\"\\nData Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d71fb64-d511-43db-bfdc-9ed1f2acf937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import sigmoid, tanh\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For building a quick neural network using nn.Sequential\n",
    "from torch.nn import Linear, Sequential, Tanh, Sigmoid, Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73aaf83d-9689-4a4d-8b62-fed9b3e26238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numeric features: 139\n",
      "Example numeric columns: ['encounter_id', 'patient_id', 'hospital_id', 'age', 'bmi', 'elective_surgery', 'height', 'icu_id', 'pre_icu_los_days', 'readmission_status']\n"
     ]
    }
   ],
   "source": [
    "# 1. Keep numeric features only (excluding the target)\n",
    "numeric_cols = [col for col in df.columns\n",
    "                if col != target_col\n",
    "                and pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "# 2. Build X and Y\n",
    "X_data = df[numeric_cols].to_numpy()\n",
    "Y_data = df[target_col].to_numpy().astype(np.float32)\n",
    "\n",
    "print(\"Number of numeric features:\", len(numeric_cols))\n",
    "print(\"Example numeric columns:\", numeric_cols[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96e806f9-08fb-437b-a617-3ceb8b9980f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_data,\n",
    "    Y_data,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=Y_data  # Keep the same 0/1 ratio in train/test\n",
    ")\n",
    "\n",
    "# 4. Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "febef311-e74f-463b-988d-82d3fe2384e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = t.tensor(X_train, dtype=t.float32)\n",
    "Y_train_t = t.tensor(Y_train, dtype=t.float32)\n",
    "\n",
    "X_test_t = t.tensor(X_test, dtype=t.float32)\n",
    "Y_test_t = t.tensor(Y_test, dtype=t.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "984aecea-24f3-4c89-b425-94e7c64b5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_t, Y_train_t)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5601441b-a51f-4c2c-848b-a29ccbeec11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=139, out_features=16, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (4): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#  Define your model\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_units = 16  # Adjust as needed\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(input_dim, hidden_units),\n",
    "    Tanh(),        # or ReLU()\n",
    "    Dropout(0.2),  # 20% dropout, tune as appropriate\n",
    "    Linear(hidden_units, 1),\n",
    "    Sigmoid()      # final activation for binary classification\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6dfba62b-b72f-4a03-9ec8-e9c75e4cd604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: BCELoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# S  Define the loss function and optimizer\n",
    "\n",
    "loss_fn = nn.BCELoss()            # for binary classification, expects outputs in [0,1]\n",
    "learning_rate = 0.01\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Loss function:\", loss_fn)\n",
    "print(\"Optimizer:\", optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "405d2b67-e045-4a27-8d52-b4a65f2db0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set epochs = 50\n",
      "Initialized train_loss_list, train_acc_list, val_acc_list.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Set up training parameters and metric lists\n",
    "\n",
    "epochs = 50  # You can adjust this\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "print(\"Set epochs =\", epochs)\n",
    "print(\"Initialized train_loss_list, train_acc_list, val_acc_list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e7e0a8f4-cd5a-4bb5-a0a4-8c561256d175",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m probs \u001b[38;5;241m=\u001b[39m model(X_batch)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape: (batch_size,)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 2) Compute loss\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 3) Backward & optimize\u001b[39;00m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:699\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3569\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3566\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3567\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "# S  Partial Training Loop (First 5 Batches)\n",
    "\n",
    "model.train()  # set to training mode\n",
    "epoch_losses = []  # We'll store the per-batch losses temporarily here\n",
    "\n",
    "for i, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "    # 1) Forward pass\n",
    "    probs = model(X_batch).view(-1)  # shape: (batch_size,)\n",
    "\n",
    "    # 2) Compute loss\n",
    "    loss = loss_fn(probs, Y_batch)\n",
    "\n",
    "    # 3) Backward & optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 4) Store batch loss\n",
    "    epoch_losses.append(loss.item())\n",
    "\n",
    "    # Just process the first 5 batches, then stop\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "print(f\"Processed 5 batches. Collected {len(epoch_losses)} losses.\")\n",
    "print(\"Losses from these batches:\", epoch_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5ac695ca-981d-4e9d-ab9f-446ae32b8cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Batch 0 ===\n",
      "Model output range -> min: nan max: nan\n",
      "Target unique values: tensor([0.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget unique values:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Y_batch\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 2) Compute loss\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# <-- might throw error if out of range\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 3) Backward & optimize\u001b[39;00m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:699\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3569\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3566\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3567\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "#1) Rerun the Partial Loop with Diagnostics \n",
    "model.train()  # ensure we are in training mode\n",
    "epoch_losses = []\n",
    "\n",
    "# We'll do 5 batches again, but with the debug prints.\n",
    "for i, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "    # 1) Forward pass\n",
    "    probs = model(X_batch).view(-1)\n",
    "\n",
    "    # 1a) DEBUG: Print ranges\n",
    "    print(f\"\\n=== Batch {i} ===\")\n",
    "    print(\"Model output range -> min:\", probs.min().item(), \"max:\", probs.max().item())\n",
    "    print(\"Target unique values:\", Y_batch.unique())\n",
    "\n",
    "    # 2) Compute loss\n",
    "    loss = loss_fn(probs, Y_batch)  # <-- might throw error if out of range\n",
    "\n",
    "    # 3) Backward & optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_losses.append(loss.item())\n",
    "\n",
    "    # Stop after 5 batches\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "print(f\"\\nCollected {len(epoch_losses)} losses:\", epoch_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "073e5ed2-5c21-44bd-aa7e-f430e039b1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaNs in X_train: 3536117\n",
      "Total NaNs in X_test: 1515142\n",
      "Columns with NaNs in X_train: [  3   4   6  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24\n",
      "  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42\n",
      "  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114\n",
      " 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132\n",
      " 133 134 135 136 137 138]\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any NaNs in X_train or X_test\n",
    "import numpy as np\n",
    "\n",
    "nan_train = np.isnan(X_train)\n",
    "nan_test = np.isnan(X_test)\n",
    "\n",
    "print(\"Total NaNs in X_train:\", nan_train.sum())\n",
    "print(\"Total NaNs in X_test:\", nan_test.sum())\n",
    "\n",
    "# If you want to see which columns have NaNs:\n",
    "cols_with_nan = np.where(np.isnan(X_train).any(axis=0))[0]\n",
    "print(\"Columns with NaNs in X_train:\", cols_with_nan) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5fa312fd-048a-4aba-a82c-2a2f54006226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done mean imputation on X_train, X_test.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step A1: Create an imputer that replaces NaN with the mean of each column\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Step A2: Fit imputer on X_train (and transform X_train)\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Step A3: Transform X_test using the same statistics\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "print(\"Done mean imputation on X_train, X_test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "32b9024f-efb7-4327-aa70-6bcce7df372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_imputed\n",
    "X_test = X_test_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5b20885d-7963-4ffa-82cc-f584d6fd09f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in X_train_imputed: 0\n",
      "NaNs in X_test_imputed: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"NaNs in X_train_imputed:\", np.isnan(X_train_imputed).sum())\n",
    "print(\"NaNs in X_test_imputed:\", np.isnan(X_test_imputed).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "847c2bc2-9688-4892-9ea4-b48238e093c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e4b0e665-3e6e-4aca-94f6-5adad4b73b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step D.1 done: Tensors & train_loader created.\n",
      "X_train_t shape: torch.Size([64199, 139])\n",
      "Y_train_t shape: torch.Size([64199])\n"
     ]
    }
   ],
   "source": [
    "# Step D.1: Convert data to PyTorch tensors and build a DataLoader\n",
    "\n",
    "import torch\n",
    "\n",
    "# Convert arrays to float32 tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_t = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_t = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset for training data\n",
    "train_dataset = TensorDataset(X_train_t, Y_train_t)\n",
    "\n",
    "# Define batch size and DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Step D.1 done: Tensors & train_loader created.\")\n",
    "print(\"X_train_t shape:\", X_train_t.shape)\n",
    "print(\"Y_train_t shape:\", Y_train_t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cc0d1777-015a-479f-8e77-eb36e340bcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in final X_train: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"NaNs in final X_train:\", np.isnan(X_train).sum())\n",
    "# This must be 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ec8a1157-9acc-4460-b0c6-a40cefdbe3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero variance columns: [9]\n"
     ]
    }
   ],
   "source": [
    "# Suppose X_train_imputed is your fully imputed data\n",
    "stds = X_train_imputed.std(axis=0)\n",
    "zero_var_cols = np.where(stds == 0)[0]\n",
    "print(\"Zero variance columns:\", zero_var_cols)\n",
    "\n",
    "# If there are any zero-variance columns, drop them:\n",
    "X_train_imputed = np.delete(X_train_imputed, zero_var_cols, axis=1)\n",
    "X_test_imputed = np.delete(X_test_imputed, zero_var_cols, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "569a56e1-d61a-41be-80cf-0bcca04b8370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in X_train after impute & scale: 0\n",
      "NaN in X_test after impute & scale: 0\n"
     ]
    }
   ],
   "source": [
    "# Final check\n",
    "print(\"NaN in X_train after impute & scale:\", np.isnan(X_train).sum())\n",
    "print(\"NaN in X_test after impute & scale:\", np.isnan(X_test).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdab755-74f3-4b66-adb1-9ddac6d77881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "25363aac-b85b-44e7-a4d0-0ff2c67f79fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model redefined:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=139, out_features=16, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (4): Sigmoid()\n",
      ")\n",
      "\n",
      "Loss function: BCELoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step D.2: Define or re-initialize the model, loss, and optimizer\n",
    "\n",
    "input_dim = X_train_t.shape[1]  # number of features\n",
    "hidden_units = 16               # you can tweak this\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(input_dim, hidden_units),\n",
    "    Tanh(),\n",
    "    Dropout(0.2),\n",
    "    Linear(hidden_units, 1),\n",
    "    Sigmoid()\n",
    ")\n",
    "\n",
    "print(\"Model redefined:\\n\", model)\n",
    "\n",
    "# Define BCELoss for binary classification\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Choose an optimizer, e.g. Adam\n",
    "learning_rate = 0.01\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"\\nLoss function:\", loss_fn)\n",
    "print(\"Optimizer:\", optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b85c9867-a4ea-497c-8269-71480ec2c2a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m probs \u001b[38;5;241m=\u001b[39m model(X_batch)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape: (batch_size,)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 2) Compute loss\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 3) Zero gradients, backward pass, update\u001b[39;00m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:699\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3569\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3566\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3567\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "# Step D.3: Partial training loop (first 5 batches)\n",
    "\n",
    "model.train()  # set to training mode\n",
    "epoch_losses = []\n",
    "\n",
    "for i, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "    # 1) Forward pass\n",
    "    probs = model(X_batch).view(-1)  # shape: (batch_size,)\n",
    "\n",
    "    # 2) Compute loss\n",
    "    loss = loss_fn(probs, Y_batch)\n",
    "\n",
    "    # 3) Zero gradients, backward pass, update\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 4) Store batch loss\n",
    "    epoch_losses.append(loss.item())\n",
    "\n",
    "    # Just do the first 5 batches\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "print(f\"Processed {len(epoch_losses)} batches. Losses:\\n\", epoch_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b50ddb5b-d4a3-46c9-a765-fcc56f512420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Debugging batch 0 ===\n",
      "X_batch min: nan X_batch max: nan X_batch has_nan: True\n",
      "Model output shape: torch.Size([32, 1])\n",
      "Model output min: nan max: nan has_nan: True\n",
      "Y_batch unique: tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Step D.4: Forward-pass debug snippet (no loss computation)\n",
    "\n",
    "model.train()  # or model.eval(), doesn't matter for this quick check\n",
    "for i, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "    with torch.no_grad():\n",
    "        # Check the input batch stats\n",
    "        print(f\"\\n=== Debugging batch {i} ===\")\n",
    "        print(\"X_batch min:\", X_batch.min().item(), \n",
    "              \"X_batch max:\", X_batch.max().item(), \n",
    "              \"X_batch has_nan:\", X_batch.isnan().any().item())\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(X_batch)\n",
    "        \n",
    "        # Check the model output stats\n",
    "        print(\"Model output shape:\", out.shape)\n",
    "        print(\"Model output min:\", out.min().item(), \n",
    "              \"max:\", out.max().item(), \n",
    "              \"has_nan:\", out.isnan().any().item())\n",
    "        \n",
    "        # Check the target\n",
    "        print(\"Y_batch unique:\", Y_batch.unique())\n",
    "        \n",
    "    break  # stop after first batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1915b-9fd2-461b-a4f4-bb33b016bb65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
